{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxkucher/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 219.5ms\n",
      "Speed: 4.0ms preprocess, 219.5ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 128.7ms\n",
      "Speed: 17.4ms preprocess, 128.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 1 frisbee, 130.0ms\n",
      "Speed: 1.7ms preprocess, 130.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 1 frisbee, 143.1ms\n",
      "Speed: 4.2ms preprocess, 143.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 1 frisbee, 206.4ms\n",
      "Speed: 1.8ms preprocess, 206.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 frisbee, 116.6ms\n",
      "Speed: 1.6ms preprocess, 116.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 frisbee, 142.8ms\n",
      "Speed: 1.4ms preprocess, 142.8ms inference, 9.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 frisbee, 135.2ms\n",
      "Speed: 1.5ms preprocess, 135.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 136.9ms\n",
      "Speed: 2.0ms preprocess, 136.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 frisbee, 224.1ms\n",
      "Speed: 2.7ms preprocess, 224.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 frisbee, 141.8ms\n",
      "Speed: 4.1ms preprocess, 141.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 132.2ms\n",
      "Speed: 1.5ms preprocess, 132.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 149.8ms\n",
      "Speed: 2.2ms preprocess, 149.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 291.6ms\n",
      "Speed: 1.2ms preprocess, 291.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 154.4ms\n",
      "Speed: 4.3ms preprocess, 154.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 143.6ms\n",
      "Speed: 3.7ms preprocess, 143.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 131.1ms\n",
      "Speed: 1.8ms preprocess, 131.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 131.4ms\n",
      "Speed: 1.2ms preprocess, 131.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 129.1ms\n",
      "Speed: 1.6ms preprocess, 129.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 131.2ms\n",
      "Speed: 1.4ms preprocess, 131.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 umbrella, 139.8ms\n",
      "Speed: 1.3ms preprocess, 139.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 307.0ms\n",
      "Speed: 1.4ms preprocess, 307.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 120.4ms\n",
      "Speed: 5.1ms preprocess, 120.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 137.5ms\n",
      "Speed: 1.6ms preprocess, 137.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 134.8ms\n",
      "Speed: 1.5ms preprocess, 134.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 139.5ms\n",
      "Speed: 2.8ms preprocess, 139.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 umbrella, 146.8ms\n",
      "Speed: 1.6ms preprocess, 146.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 140.9ms\n",
      "Speed: 1.3ms preprocess, 140.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 umbrella, 139.9ms\n",
      "Speed: 1.6ms preprocess, 139.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 167.4ms\n",
      "Speed: 1.5ms preprocess, 167.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 126.6ms\n",
      "Speed: 1.7ms preprocess, 126.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 1 umbrella, 134.6ms\n",
      "Speed: 1.3ms preprocess, 134.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 1 umbrella, 137.0ms\n",
      "Speed: 1.6ms preprocess, 137.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 135.7ms\n",
      "Speed: 2.4ms preprocess, 135.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 bus, 1 truck, 1 umbrella, 131.9ms\n",
      "Speed: 4.1ms preprocess, 131.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 1 truck, 1 umbrella, 128.7ms\n",
      "Speed: 1.5ms preprocess, 128.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 1 umbrella, 112.3ms\n",
      "Speed: 16.1ms preprocess, 112.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 umbrella, 132.8ms\n",
      "Speed: 1.6ms preprocess, 132.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 umbrella, 140.7ms\n",
      "Speed: 1.6ms preprocess, 140.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 umbrella, 139.1ms\n",
      "Speed: 1.6ms preprocess, 139.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 bus, 1 truck, 1 umbrella, 140.9ms\n",
      "Speed: 1.3ms preprocess, 140.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 1 truck, 1 umbrella, 137.6ms\n",
      "Speed: 3.1ms preprocess, 137.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 bus, 1 truck, 1 umbrella, 132.2ms\n",
      "Speed: 1.4ms preprocess, 132.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 127.7ms\n",
      "Speed: 1.4ms preprocess, 127.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 bus, 1 truck, 1 umbrella, 139.7ms\n",
      "Speed: 1.8ms preprocess, 139.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 bus, 1 umbrella, 158.5ms\n",
      "Speed: 12.3ms preprocess, 158.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 1 bus, 1 umbrella, 124.2ms\n",
      "Speed: 1.4ms preprocess, 124.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7 cars, 1 bus, 1 umbrella, 127.8ms\n",
      "Speed: 1.6ms preprocess, 127.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 1 bus, 1 umbrella, 131.4ms\n",
      "Speed: 13.9ms preprocess, 131.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 umbrella, 165.6ms\n",
      "Speed: 1.3ms preprocess, 165.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 bus, 1 umbrella, 134.6ms\n",
      "Speed: 13.4ms preprocess, 134.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 1 bus, 1 umbrella, 135.3ms\n",
      "Speed: 1.6ms preprocess, 135.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 bus, 1 umbrella, 147.9ms\n",
      "Speed: 1.5ms preprocess, 147.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 1 umbrella, 133.5ms\n",
      "Speed: 1.5ms preprocess, 133.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 11 cars, 1 umbrella, 181.7ms\n",
      "Speed: 1.6ms preprocess, 181.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 umbrella, 140.0ms\n",
      "Speed: 1.2ms preprocess, 140.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 133.0ms\n",
      "Speed: 2.6ms preprocess, 133.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 1 umbrella, 136.7ms\n",
      "Speed: 1.4ms preprocess, 136.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 umbrella, 133.1ms\n",
      "Speed: 1.8ms preprocess, 133.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 127.2ms\n",
      "Speed: 1.8ms preprocess, 127.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 1 umbrella, 136.3ms\n",
      "Speed: 1.6ms preprocess, 136.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 132.6ms\n",
      "Speed: 1.5ms preprocess, 132.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 128.3ms\n",
      "Speed: 1.4ms preprocess, 128.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 143.9ms\n",
      "Speed: 1.8ms preprocess, 143.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 1 umbrella, 131.1ms\n",
      "Speed: 1.5ms preprocess, 131.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 umbrella, 133.4ms\n",
      "Speed: 1.4ms preprocess, 133.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 126.1ms\n",
      "Speed: 3.8ms preprocess, 126.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 122.0ms\n",
      "Speed: 1.6ms preprocess, 122.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 129.6ms\n",
      "Speed: 1.7ms preprocess, 129.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 1 umbrella, 130.0ms\n",
      "Speed: 1.9ms preprocess, 130.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 umbrella, 142.1ms\n",
      "Speed: 1.6ms preprocess, 142.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 140.0ms\n",
      "Speed: 1.7ms preprocess, 140.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 umbrella, 151.8ms\n",
      "Speed: 1.4ms preprocess, 151.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 1 umbrella, 160.3ms\n",
      "Speed: 1.5ms preprocess, 160.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 umbrella, 151.6ms\n",
      "Speed: 3.0ms preprocess, 151.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 148.4ms\n",
      "Speed: 2.7ms preprocess, 148.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 umbrella, 137.4ms\n",
      "Speed: 2.2ms preprocess, 137.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 138.9ms\n",
      "Speed: 1.2ms preprocess, 138.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 1 umbrella, 144.7ms\n",
      "Speed: 1.8ms preprocess, 144.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 146.9ms\n",
      "Speed: 1.3ms preprocess, 146.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 166.2ms\n",
      "Speed: 1.4ms preprocess, 166.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 1 umbrella, 166.6ms\n",
      "Speed: 1.9ms preprocess, 166.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 umbrella, 148.4ms\n",
      "Speed: 1.9ms preprocess, 148.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 146.4ms\n",
      "Speed: 1.3ms preprocess, 146.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 145.4ms\n",
      "Speed: 1.4ms preprocess, 145.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 125.0ms\n",
      "Speed: 1.3ms preprocess, 125.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 umbrella, 164.3ms\n",
      "Speed: 1.3ms preprocess, 164.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 1 umbrella, 135.4ms\n",
      "Speed: 2.4ms preprocess, 135.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 1 umbrella, 132.6ms\n",
      "Speed: 1.4ms preprocess, 132.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 1 umbrella, 131.4ms\n",
      "Speed: 1.2ms preprocess, 131.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 2 trucks, 1 umbrella, 127.7ms\n",
      "Speed: 1.6ms preprocess, 127.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 4 trucks, 1 umbrella, 120.2ms\n",
      "Speed: 1.9ms preprocess, 120.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 163.5ms\n",
      "Speed: 12.8ms preprocess, 163.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 250.0ms\n",
      "Speed: 2.8ms preprocess, 250.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 truck, 1 umbrella, 161.6ms\n",
      "Speed: 1.4ms preprocess, 161.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 truck, 1 umbrella, 142.5ms\n",
      "Speed: 1.5ms preprocess, 142.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 170.3ms\n",
      "Speed: 2.6ms preprocess, 170.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 truck, 1 umbrella, 141.2ms\n",
      "Speed: 1.3ms preprocess, 141.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 2 trucks, 1 umbrella, 182.6ms\n",
      "Speed: 2.8ms preprocess, 182.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 167.0ms\n",
      "Speed: 1.7ms preprocess, 167.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 10 cars, 1 truck, 1 umbrella, 164.1ms\n",
      "Speed: 2.6ms preprocess, 164.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 12 cars, 1 umbrella, 140.6ms\n",
      "Speed: 2.2ms preprocess, 140.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 truck, 1 umbrella, 130.4ms\n",
      "Speed: 1.4ms preprocess, 130.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 umbrella, 152.4ms\n",
      "Speed: 3.4ms preprocess, 152.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 umbrella, 165.8ms\n",
      "Speed: 1.5ms preprocess, 165.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 umbrella, 132.5ms\n",
      "Speed: 2.0ms preprocess, 132.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 13 cars, 1 bus, 1 umbrella, 121.2ms\n",
      "Speed: 1.5ms preprocess, 121.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 umbrella, 131.8ms\n",
      "Speed: 2.1ms preprocess, 131.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 umbrella, 146.8ms\n",
      "Speed: 1.3ms preprocess, 146.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 umbrella, 130.0ms\n",
      "Speed: 1.6ms preprocess, 130.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 umbrella, 196.9ms\n",
      "Speed: 3.6ms preprocess, 196.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 umbrella, 256.5ms\n",
      "Speed: 4.1ms preprocess, 256.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 bus, 1 umbrella, 119.1ms\n",
      "Speed: 1.1ms preprocess, 119.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 umbrella, 114.6ms\n",
      "Speed: 3.9ms preprocess, 114.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 umbrella, 125.0ms\n",
      "Speed: 2.0ms preprocess, 125.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 umbrella, 131.1ms\n",
      "Speed: 1.5ms preprocess, 131.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 cars, 1 umbrella, 143.9ms\n",
      "Speed: 2.0ms preprocess, 143.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 umbrella, 1 frisbee, 152.0ms\n",
      "Speed: 6.5ms preprocess, 152.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 truck, 1 umbrella, 130.2ms\n",
      "Speed: 1.4ms preprocess, 130.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 truck, 1 umbrella, 135.1ms\n",
      "Speed: 2.3ms preprocess, 135.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 1 umbrella, 137.8ms\n",
      "Speed: 1.7ms preprocess, 137.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 truck, 1 umbrella, 129.7ms\n",
      "Speed: 1.2ms preprocess, 129.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 2 trucks, 1 umbrella, 164.7ms\n",
      "Speed: 1.7ms preprocess, 164.7ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 2 trucks, 1 umbrella, 1 frisbee, 405.1ms\n",
      "Speed: 5.3ms preprocess, 405.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 1 umbrella, 1 frisbee, 126.2ms\n",
      "Speed: 4.5ms preprocess, 126.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 1 umbrella, 1 frisbee, 126.0ms\n",
      "Speed: 1.5ms preprocess, 126.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 1 umbrella, 1 frisbee, 137.8ms\n",
      "Speed: 1.4ms preprocess, 137.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 1 umbrella, 133.5ms\n",
      "Speed: 1.8ms preprocess, 133.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 truck, 1 umbrella, 1 frisbee, 136.6ms\n",
      "Speed: 2.4ms preprocess, 136.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 1 umbrella, 132.5ms\n",
      "Speed: 1.6ms preprocess, 132.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 1 umbrella, 122.6ms\n",
      "Speed: 1.9ms preprocess, 122.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 truck, 1 umbrella, 128.4ms\n",
      "Speed: 1.5ms preprocess, 128.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 truck, 1 umbrella, 127.3ms\n",
      "Speed: 2.0ms preprocess, 127.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 truck, 1 umbrella, 1 frisbee, 129.8ms\n",
      "Speed: 1.9ms preprocess, 129.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 11 cars, 1 bus, 1 truck, 1 umbrella, 1 frisbee, 138.0ms\n",
      "Speed: 1.2ms preprocess, 138.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 11 cars, 1 bus, 1 truck, 1 umbrella, 158.1ms\n",
      "Speed: 1.8ms preprocess, 158.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 bus, 1 umbrella, 1 frisbee, 123.0ms\n",
      "Speed: 3.1ms preprocess, 123.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 1 bus, 1 umbrella, 1 frisbee, 136.5ms\n",
      "Speed: 1.9ms preprocess, 136.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 11 cars, 1 bus, 1 truck, 1 umbrella, 130.5ms\n",
      "Speed: 1.6ms preprocess, 130.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 bus, 1 umbrella, 1 frisbee, 137.1ms\n",
      "Speed: 1.5ms preprocess, 137.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 2 buss, 1 umbrella, 1 frisbee, 131.0ms\n",
      "Speed: 2.2ms preprocess, 131.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 2 buss, 1 umbrella, 129.7ms\n",
      "Speed: 1.9ms preprocess, 129.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 2 buss, 1 umbrella, 1 frisbee, 564.7ms\n",
      "Speed: 1.4ms preprocess, 564.7ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 2 buss, 1 umbrella, 1 frisbee, 196.5ms\n",
      "Speed: 7.9ms preprocess, 196.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 2 buss, 1 umbrella, 1 frisbee, 115.0ms\n",
      "Speed: 1.7ms preprocess, 115.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 buss, 1 umbrella, 1 frisbee, 170.9ms\n",
      "Speed: 1.8ms preprocess, 170.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 2 buss, 1 umbrella, 1 frisbee, 121.1ms\n",
      "Speed: 1.8ms preprocess, 121.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 buss, 1 umbrella, 1 frisbee, 145.9ms\n",
      "Speed: 1.4ms preprocess, 145.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 buss, 1 truck, 1 umbrella, 1 frisbee, 132.1ms\n",
      "Speed: 1.8ms preprocess, 132.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 bus, 1 truck, 1 umbrella, 1 frisbee, 158.9ms\n",
      "Speed: 1.4ms preprocess, 158.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 2 buss, 2 trucks, 1 umbrella, 129.4ms\n",
      "Speed: 1.7ms preprocess, 129.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 2 trucks, 1 umbrella, 127.4ms\n",
      "Speed: 2.5ms preprocess, 127.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 truck, 1 umbrella, 136.7ms\n",
      "Speed: 1.6ms preprocess, 136.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 134.1ms\n",
      "Speed: 1.6ms preprocess, 134.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 1 umbrella, 1 frisbee, 135.2ms\n",
      "Speed: 3.3ms preprocess, 135.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 1 frisbee, 141.6ms\n",
      "Speed: 2.2ms preprocess, 141.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 1 frisbee, 134.3ms\n",
      "Speed: 1.8ms preprocess, 134.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 1 frisbee, 123.3ms\n",
      "Speed: 1.7ms preprocess, 123.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 1 frisbee, 137.5ms\n",
      "Speed: 1.2ms preprocess, 137.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 1 frisbee, 147.6ms\n",
      "Speed: 1.8ms preprocess, 147.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 frisbee, 124.5ms\n",
      "Speed: 1.6ms preprocess, 124.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 1 umbrella, 1 frisbee, 143.0ms\n",
      "Speed: 1.6ms preprocess, 143.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 1 umbrella, 1 frisbee, 134.8ms\n",
      "Speed: 6.8ms preprocess, 134.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 umbrella, 1 frisbee, 123.7ms\n",
      "Speed: 1.5ms preprocess, 123.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 1 truck, 1 umbrella, 1 frisbee, 141.2ms\n",
      "Speed: 1.4ms preprocess, 141.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 truck, 1 umbrella, 1 frisbee, 141.8ms\n",
      "Speed: 2.4ms preprocess, 141.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 umbrella, 1 frisbee, 124.2ms\n",
      "Speed: 1.3ms preprocess, 124.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 frisbee, 136.9ms\n",
      "Speed: 1.1ms preprocess, 136.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 frisbee, 361.6ms\n",
      "Speed: 42.8ms preprocess, 361.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 frisbee, 139.6ms\n",
      "Speed: 2.7ms preprocess, 139.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 frisbee, 120.8ms\n",
      "Speed: 2.1ms preprocess, 120.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 frisbee, 116.3ms\n",
      "Speed: 1.7ms preprocess, 116.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 cars, 1 frisbee, 144.1ms\n",
      "Speed: 1.5ms preprocess, 144.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 cars, 1 frisbee, 133.6ms\n",
      "Speed: 1.5ms preprocess, 133.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 cars, 1 frisbee, 115.5ms\n",
      "Speed: 2.3ms preprocess, 115.5ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 frisbee, 139.3ms\n",
      "Speed: 2.1ms preprocess, 139.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 cars, 1 frisbee, 179.3ms\n",
      "Speed: 1.2ms preprocess, 179.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 11 cars, 1 frisbee, 176.8ms\n",
      "Speed: 4.5ms preprocess, 176.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12 cars, 1 frisbee, 114.0ms\n",
      "Speed: 1.6ms preprocess, 114.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10 cars, 1 frisbee, 134.7ms\n",
      "Speed: 1.5ms preprocess, 134.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9 cars, 1 frisbee, 128.7ms\n",
      "Speed: 1.7ms preprocess, 128.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 frisbee, 154.5ms\n",
      "Speed: 1.4ms preprocess, 154.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 umbrella, 1 frisbee, 130.8ms\n",
      "Speed: 1.6ms preprocess, 130.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 frisbee, 141.3ms\n",
      "Speed: 1.3ms preprocess, 141.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 frisbee, 105.6ms\n",
      "Speed: 1.6ms preprocess, 105.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 frisbee, 120.4ms\n",
      "Speed: 1.5ms preprocess, 120.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 frisbee, 125.8ms\n",
      "Speed: 1.4ms preprocess, 125.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 frisbee, 114.9ms\n",
      "Speed: 1.2ms preprocess, 114.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 frisbee, 115.3ms\n",
      "Speed: 3.9ms preprocess, 115.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 frisbee, 143.7ms\n",
      "Speed: 2.7ms preprocess, 143.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 frisbee, 131.7ms\n",
      "Speed: 1.5ms preprocess, 131.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 frisbee, 133.6ms\n",
      "Speed: 1.5ms preprocess, 133.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 frisbee, 135.7ms\n",
      "Speed: 2.9ms preprocess, 135.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7 cars, 1 truck, 1 frisbee, 133.5ms\n",
      "Speed: 3.1ms preprocess, 133.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 1 frisbee, 159.1ms\n",
      "Speed: 2.6ms preprocess, 159.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 1 frisbee, 146.3ms\n",
      "Speed: 1.7ms preprocess, 146.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 1 truck, 1 frisbee, 177.6ms\n",
      "Speed: 1.8ms preprocess, 177.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 1 frisbee, 120.2ms\n",
      "Speed: 1.7ms preprocess, 120.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 frisbee, 156.3ms\n",
      "Speed: 3.0ms preprocess, 156.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 frisbee, 123.8ms\n",
      "Speed: 1.5ms preprocess, 123.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 1 frisbee, 117.4ms\n",
      "Speed: 1.8ms preprocess, 117.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 1 frisbee, 130.3ms\n",
      "Speed: 1.2ms preprocess, 130.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 frisbee, 135.1ms\n",
      "Speed: 1.6ms preprocess, 135.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 frisbee, 126.4ms\n",
      "Speed: 2.1ms preprocess, 126.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 frisbee, 143.1ms\n",
      "Speed: 1.7ms preprocess, 143.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 frisbee, 126.3ms\n",
      "Speed: 1.2ms preprocess, 126.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 frisbee, 132.3ms\n",
      "Speed: 1.8ms preprocess, 132.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 frisbee, 129.2ms\n",
      "Speed: 1.3ms preprocess, 129.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 1 frisbee, 126.5ms\n",
      "Speed: 2.4ms preprocess, 126.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 frisbee, 126.2ms\n",
      "Speed: 2.2ms preprocess, 126.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 1 frisbee, 134.6ms\n",
      "Speed: 2.6ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 frisbee, 154.0ms\n",
      "Speed: 4.6ms preprocess, 154.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6 cars, 1 truck, 1 frisbee, 127.4ms\n",
      "Speed: 2.2ms preprocess, 127.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 1 frisbee, 126.6ms\n",
      "Speed: 1.6ms preprocess, 126.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 1 frisbee, 131.8ms\n",
      "Speed: 2.1ms preprocess, 131.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 frisbee, 129.0ms\n",
      "Speed: 1.4ms preprocess, 129.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 frisbee, 124.2ms\n",
      "Speed: 43.4ms preprocess, 124.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 frisbee, 146.5ms\n",
      "Speed: 1.0ms preprocess, 146.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 frisbee, 141.3ms\n",
      "Speed: 1.5ms preprocess, 141.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 1 truck, 1 frisbee, 140.4ms\n",
      "Speed: 1.4ms preprocess, 140.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 1 truck, 1 frisbee, 139.7ms\n",
      "Speed: 1.5ms preprocess, 139.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 1 truck, 1 frisbee, 173.1ms\n",
      "Speed: 3.0ms preprocess, 173.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 1 truck, 1 frisbee, 154.7ms\n",
      "Speed: 7.4ms preprocess, 154.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6 cars, 1 truck, 1 frisbee, 123.5ms\n",
      "Speed: 1.3ms preprocess, 123.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 truck, 1 frisbee, 132.6ms\n",
      "Speed: 3.0ms preprocess, 132.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 truck, 1 frisbee, 277.7ms\n",
      "Speed: 1.8ms preprocess, 277.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 truck, 1 frisbee, 119.5ms\n",
      "Speed: 1.7ms preprocess, 119.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 1 truck, 1 frisbee, 118.9ms\n",
      "Speed: 1.5ms preprocess, 118.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 1 truck, 1 frisbee, 1 toilet, 152.0ms\n",
      "Speed: 1.0ms preprocess, 152.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 1 truck, 1 frisbee, 135.6ms\n",
      "Speed: 2.0ms preprocess, 135.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 frisbee, 182.4ms\n",
      "Speed: 1.8ms preprocess, 182.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4 cars, 1 truck, 1 frisbee, 128.7ms\n",
      "Speed: 2.5ms preprocess, 128.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 1 truck, 1 frisbee, 112.2ms\n",
      "Speed: 1.6ms preprocess, 112.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 4 cars, 1 truck, 1 frisbee, 128.9ms\n",
      "Speed: 1.7ms preprocess, 128.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 frisbee, 145.4ms\n",
      "Speed: 1.6ms preprocess, 145.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 1 frisbee, 136.0ms\n",
      "Speed: 1.4ms preprocess, 136.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 1 frisbee, 139.0ms\n",
      "Speed: 1.3ms preprocess, 139.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 5 cars, 1 truck, 1 frisbee, 123.0ms\n",
      "Speed: 1.5ms preprocess, 123.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 frisbee, 135.1ms\n",
      "Speed: 1.3ms preprocess, 135.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 1 frisbee, 120.5ms\n",
      "Speed: 1.6ms preprocess, 120.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 umbrella, 148.6ms\n",
      "Speed: 3.2ms preprocess, 148.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 1 umbrella, 122.7ms\n",
      "Speed: 1.7ms preprocess, 122.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 1 umbrella, 1 frisbee, 130.2ms\n",
      "Speed: 1.9ms preprocess, 130.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 1 umbrella, 177.0ms\n",
      "Speed: 1.5ms preprocess, 177.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 2 trucks, 1 umbrella, 1 frisbee, 135.1ms\n",
      "Speed: 1.2ms preprocess, 135.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 1 umbrella, 1 frisbee, 132.6ms\n",
      "Speed: 1.5ms preprocess, 132.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 1 umbrella, 129.7ms\n",
      "Speed: 1.6ms preprocess, 129.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 1 umbrella, 130.4ms\n",
      "Speed: 7.6ms preprocess, 130.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 1 truck, 1 umbrella, 145.1ms\n",
      "Speed: 1.3ms preprocess, 145.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 131.1ms\n",
      "Speed: 1.2ms preprocess, 131.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 124.5ms\n",
      "Speed: 1.4ms preprocess, 124.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 143.0ms\n",
      "Speed: 1.4ms preprocess, 143.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 cars, 1 truck, 1 umbrella, 153.8ms\n",
      "Speed: 1.5ms preprocess, 153.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 164.9ms\n",
      "Speed: 1.9ms preprocess, 164.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 126.3ms\n",
      "Speed: 1.4ms preprocess, 126.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 1 umbrella, 139.6ms\n",
      "Speed: 1.7ms preprocess, 139.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 141.8ms\n",
      "Speed: 1.9ms preprocess, 141.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 138.1ms\n",
      "Speed: 1.9ms preprocess, 138.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 1 frisbee, 141.2ms\n",
      "Speed: 1.6ms preprocess, 141.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 2 trucks, 1 umbrella, 1 frisbee, 179.2ms\n",
      "Speed: 1.4ms preprocess, 179.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 1 frisbee, 143.9ms\n",
      "Speed: 1.6ms preprocess, 143.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 1 frisbee, 143.3ms\n",
      "Speed: 2.2ms preprocess, 143.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 1 frisbee, 155.4ms\n",
      "Speed: 1.7ms preprocess, 155.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 1 frisbee, 156.7ms\n",
      "Speed: 1.5ms preprocess, 156.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 1 frisbee, 167.3ms\n",
      "Speed: 1.4ms preprocess, 167.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 1 frisbee, 157.6ms\n",
      "Speed: 1.7ms preprocess, 157.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 cars, 1 truck, 1 umbrella, 1 frisbee, 168.9ms\n",
      "Speed: 1.7ms preprocess, 168.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 2 trucks, 1 umbrella, 1 frisbee, 157.8ms\n",
      "Speed: 1.8ms preprocess, 157.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 1 frisbee, 255.9ms\n",
      "Speed: 1.3ms preprocess, 255.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 1 frisbee, 140.0ms\n",
      "Speed: 1.9ms preprocess, 140.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 cars, 1 truck, 1 umbrella, 139.8ms\n",
      "Speed: 4.4ms preprocess, 139.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 153.3ms\n",
      "Speed: 2.1ms preprocess, 153.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 3 trucks, 1 umbrella, 139.4ms\n",
      "Speed: 1.7ms preprocess, 139.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 1 umbrella, 135.2ms\n",
      "Speed: 1.3ms preprocess, 135.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 2 trucks, 1 umbrella, 146.7ms\n",
      "Speed: 13.1ms preprocess, 146.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 2 trucks, 1 umbrella, 134.8ms\n",
      "Speed: 1.8ms preprocess, 134.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 151.2ms\n",
      "Speed: 5.3ms preprocess, 151.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 141.5ms\n",
      "Speed: 1.5ms preprocess, 141.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 1 umbrella, 151.3ms\n",
      "Speed: 1.6ms preprocess, 151.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 1 frisbee, 139.7ms\n",
      "Speed: 1.5ms preprocess, 139.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 2 trucks, 1 umbrella, 133.0ms\n",
      "Speed: 1.7ms preprocess, 133.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 132.5ms\n",
      "Speed: 2.0ms preprocess, 132.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 138.6ms\n",
      "Speed: 1.1ms preprocess, 138.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 1 umbrella, 1 frisbee, 131.2ms\n",
      "Speed: 1.4ms preprocess, 131.2ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 8 cars, 1 truck, 1 umbrella, 134.2ms\n",
      "Speed: 1.3ms preprocess, 134.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7 cars, 1 truck, 1 umbrella, 125.0ms\n",
      "Speed: 2.0ms preprocess, 125.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 1 frisbee, 129.5ms\n",
      "Speed: 1.3ms preprocess, 129.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 162.2ms\n",
      "Speed: 2.8ms preprocess, 162.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 cars, 1 truck, 1 umbrella, 1 frisbee, 190.2ms\n",
      "Speed: 1.7ms preprocess, 190.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 8 cars, 1 truck, 1 umbrella, 1 frisbee, 170.5ms\n",
      "Speed: 3.2ms preprocess, 170.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/maxkucher/python/CV/traffic_tracker.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# results -   YOLO,    \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#    - boxes,        ,   \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#       ID  \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m#   results  boxes,  [0]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m results \u001b[39m=\u001b[39m model(frame)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkucher/python/CV/traffic_tracker.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m object_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/model.py:169\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     source: Union[\u001b[39mstr\u001b[39m, Path, \u001b[39mint\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, np\u001b[39m.\u001b[39mndarray, torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    149\u001b[0m     stream: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    151\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/model.py:429\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m prompts \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor, \u001b[39m\"\u001b[39m\u001b[39mset_prompts\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/predictor.py:204\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/predictor.py:283\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 283\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(im, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    284\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39membed:\n\u001b[1;32m    285\u001b[0m         \u001b[39myield from\u001b[39;00m [preds] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(preds, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m preds  \u001b[39m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/engine/predictor.py:140\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m visualize \u001b[39m=\u001b[39m (\n\u001b[1;32m    136\u001b[0m     increment_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_dir \u001b[39m/\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstem, mkdir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mvisualize \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type\u001b[39m.\u001b[39mtensor)\n\u001b[1;32m    138\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize, embed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49membed, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    381\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize, embed\u001b[39m=\u001b[39;49membed)\n\u001b[1;32m    385\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/tasks.py:83\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/tasks.py:101\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/tasks.py:122\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 122\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m    123\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/modules/head.py:46\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Concatenates and returns predicted bounding boxes and class probabilities.\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnl):\n\u001b[0;32m---> 46\u001b[0m     x[i] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv2[i](x[i]), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv3[i](x[i])), \u001b[39m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:  \u001b[39m# Training path\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO \n",
    "\n",
    "cap = cv2.VideoCapture(\"path/to/your/video\")\n",
    "\n",
    "\n",
    "ret, frame = cap.read ()\n",
    "\n",
    "# assign index to each object \n",
    "index = 0\n",
    "\n",
    "# variable for specific object count\n",
    "count = 0\n",
    "\n",
    "# variable for total object number \n",
    "object_count = 0\n",
    "\n",
    "# model import from YOLO\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "# classes import \n",
    "names = model.model.names\n",
    "threshold = 0.5\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store the number of objects of each class\n",
    "# each object in the class_counts dictionary will first be assigned the number 0\n",
    "class_counts = {names[class_id].upper(): 0 for class_id in names.keys()}\n",
    "\n",
    "# the output of the dictionary will be like this:\n",
    "# {'PERSON': 0, 'BICYCLE': 0, 'CAR': 0, 'MOTORCYCLE': 0...} - since it contains absolutely all class names\n",
    "\n",
    "while ret:\n",
    "    ret, frame = cap.read()\n",
    "    # results - an object of the YOLO class that returns the model\n",
    "    # its first element is boxes, which contains a set of arrays with the coordinates of the bounding box, the probability of matching\n",
    "    # object to one or another class and object ID\n",
    "\n",
    "    # to select boxes from results, initialize [0]\n",
    "    # thne place it to results object\n",
    "    results = model(frame)[0]\n",
    "    \n",
    "    # Reset object counters at the beginning of each iteration\n",
    "\n",
    "    count = 0\n",
    "    object_count = 0\n",
    "    index = 0\n",
    "\n",
    "    for name in names.values():\n",
    "        class_counts[name.upper()] = 0\n",
    "\n",
    "    # results can't be iterated since it's an object, so I convert that part of it called boxes into a list\n",
    "    # in the end the result for each object will look like this:\n",
    "    # [0.361572265625, 547.4945068359375, 132.5535888671875, 714.080078125, 0.8390625715255737, 2.0] - here all data corresponds to the variables x1, y1, x2, y2, score, class_id\n",
    "    # which I put in result\n",
    "\n",
    "    # results corresponds to the number of detected objects\n",
    "    for result in results.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "\n",
    "        \n",
    "        # the keys for the class_counts dictionary are the names of the classes from the \"names\" array\n",
    "        class_name = names[int(class_id)].upper()\n",
    "        \n",
    "\n",
    "        if score > threshold:\n",
    "        \n",
    "            count += 1\n",
    "            index += 1\n",
    "            object_count += 1\n",
    "            x_center = int((x1 + x2) / 2)\n",
    "            y_center = int((y1 + y2) / 2)\n",
    "            # if an object is detected, increase the value of the corresponding key\n",
    "            # (which corresponds to class_name) by 1\n",
    "            class_counts[class_name] += 1\n",
    "            \n",
    "            # FOR EACH DETECTED OBJECT\n",
    "\n",
    "            # bounding box\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
    "            # point at the center of each detected object\n",
    "            cv2.circle(frame, (x_center, y_center), 3, (0, 0, 255), thickness=cv2.FILLED)\n",
    "            # personal number and class name of each detected object\n",
    "            cv2.putText(frame, f\"{index}: {names[int(class_id)].upper()}\", (int(x1), int(y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "     \n",
    "    \n",
    "\n",
    "    # since the class_counts dictionary contains too many classes, we make a slice in which we mark only the most important classes for us\n",
    "    # in this case it is {\"CAR\", \"PERSON\", \"MOTORCYCLE\", \"BICYCLE\", \"TRUCK\", \"BUS\"}\n",
    "            \n",
    "    class_counts = {key: class_counts[key] for key in class_counts.keys() & {\"CAR\", \"PERSON\", \"MOTORCYCLE\", \"BICYCLE\", \"TRUCK\", \"BUS\"}}\n",
    "\n",
    "    # Displaying the number of objects of each class\n",
    "    text = \"\"\n",
    "    for name, count in class_counts.items():\n",
    "        text += f\"{name}: {count} \"\n",
    "\n",
    "    cv2.rectangle(frame, (5, 660), (1400, 690), (0, 0, 0), 80)    \n",
    "    cv2.putText(frame, text, (15, 675), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # object_count - since this line displays the total number of objects\n",
    "    cv2.rectangle(frame, (8, 15), (350, 25), (0, 0, 0), 20)\n",
    "    cv2.putText(frame, f\"Total object count: {object_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    \n",
    "    \n",
    " \n",
    "    ret, frame = cap.read()\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
