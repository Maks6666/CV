{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-13T09:58:50.836957Z",
     "start_time": "2024-11-13T09:58:38.586100Z"
    }
   },
   "source": [
    "from main import track_id\n",
    "\n",
    "\n",
    "# Basic MOT realisation with SORT (sort.py)\n",
    "\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "       \n",
    "        self.capture_index = capture_index\n",
    "        \n",
    "        self.device = 'msp' if torch.backends.mps.is_available() else 'cpu'\n",
    "        \n",
    "        self.model = self.load_model()\n",
    "        \n",
    "        self.class_names = list(self.model.model.names.values())\n",
    "\n",
    "   \n",
    "\n",
    "    def load_model(self):\n",
    "       \n",
    "        model = YOLO(\"yolov8m.pt\")  \n",
    "        model.fuse()\n",
    "    \n",
    "        return model\n",
    "\n",
    "\n",
    "    def predict(self, frame):\n",
    "       \n",
    "        results = self.model(frame, verbose=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "\n",
    "    def get_results(self, results):\n",
    "        \n",
    "        detections_list = []\n",
    "        \n",
    "        # Extract detections\n",
    "        for result in results[0]:\n",
    "            # to get coordinates\n",
    "            bbox = result.boxes.xyxy.cpu().numpy()\n",
    "            # threshold analog\n",
    "            confidence = result.boxes.conf.cpu().numpy()\n",
    "            # class id \n",
    "            class_id = result.boxes.cls.cpu().numpy()\n",
    "            \n",
    "            \n",
    "            merged_detection = [bbox[0][0], bbox[0][1], bbox[0][2], bbox[0][3], confidence[0], class_id[0]]\n",
    "            \n",
    "            \n",
    "            detections_list.append(merged_detection)\n",
    "            print(detections_list)\n",
    "    \n",
    "        return np.array(detections_list)\n",
    "    \n",
    "    \n",
    "    def draw_bounding_boxes_with_id(self, img, bboxes, ids, class_id):\n",
    "        # bboxes - coordinates\n",
    "        # ids - unique index for each detected object to track it \n",
    "        # class_id - object class id \n",
    "        for bbox, id_, cls in zip(bboxes, ids, class_id):\n",
    "            \n",
    "            label = f\"ID: {id_} Class: {self.class_names[int(cls)]}\"\n",
    "\n",
    "            cv2.rectangle(img,(int(bbox[0]), int(bbox[1])),(int(bbox[2]), int(bbox[3])),(0,0,255),2)\n",
    "            cv2.putText(img, label, (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "            \n",
    "        return img\n",
    "       \n",
    "    \n",
    "    def __call__(self):\n",
    "\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        # to check is video opened \n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "        \n",
    "        \n",
    "        # SORT realisation\n",
    "        # max_age - amount of frames, during which, algorithm will remember a specific object\n",
    "        # min_hits - amount of frames, during which, algorithm will sort out all undecided objects \n",
    "        # iou_threshold - to calculate IOU between objects,which could be close to each other \n",
    "        sort = Sort(max_age=100, min_hits=8, iou_threshold=0.50)\n",
    "        \n",
    "      \n",
    "        while True:\n",
    "          \n",
    "            start_time = time()\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            assert ret\n",
    "            \n",
    "            results = self.predict(frame)\n",
    "            detections_list = self.get_results(results)\n",
    "            \n",
    "            # SORT Tracking\n",
    "            if len(detections_list) == 0:\n",
    "                detections_list = np.empty((0, 5))\n",
    "        \n",
    "            # put detections_list into sort.update to realize tracking \n",
    "            res = sort.update(detections_list)\n",
    "            # get coordinates of objects to track\n",
    "            boxes_track = res[:,:-1]\n",
    "            # get index of objects to track\n",
    "            boxes_ids = res[:,-1].astype(int)\n",
    "            # get objects id - from detections_list, NOT from res(!)\n",
    "            class_id = detections_list[:,-1]\n",
    "            \n",
    "            # draw bounding boxes\n",
    "            frame = self.draw_bounding_boxes_with_id(frame, boxes_track, boxes_ids, class_id)\n",
    "\n",
    "            end_time = time()\n",
    "            fps = 1/np.round(end_time - start_time, 2)\n",
    "             \n",
    "            cv2.putText(frame, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "            \n",
    "            cv2.imshow('YOLOv8 Detection', frame)\n",
    " \n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "    \n",
    "detector = ObjectDetection(capture_index=1)\n",
    "detector()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x512 (no detections), 288.3ms\n",
      "Speed: 4.0ms preprocess, 288.3ms inference, 3.6ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 384x640 (no detections), 410.9ms\n",
      "Speed: 1.9ms preprocess, 410.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 156.2ms\n",
      "Speed: 2.1ms preprocess, 156.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Detected face boxes: []\n",
      "No valid detections.\n",
      "\n",
      "0: 288x512 1 face, 370.7ms\n",
      "Speed: 29.4ms preprocess, 370.7ms inference, 3.3ms postprocess per image at shape (1, 3, 288, 512)\n",
      "\n",
      "0: 384x640 3 persons, 335.8ms\n",
      "Speed: 2.1ms preprocess, 335.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 132.7ms\n",
      "Speed: 2.6ms preprocess, 132.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Detected face boxes: [[648.5823974609375, 7.533645153045654, 1370.4390869140625, 734.0816040039062]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m track_id\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Basic MOT realisation with SORT (sort.py)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mObjectDetection\u001B[39;00m:\n",
      "File \u001B[0;32m~/opencv/face_2_face/main.py:62\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetected face boxes:\u001B[39m\u001B[38;5;124m\"\u001B[39m, face_boxes)\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m face_boxes:\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;66;03m# Обновление трекера\u001B[39;00m\n\u001B[0;32m---> 62\u001B[0m     tracks \u001B[38;5;241m=\u001B[39m tracker\u001B[38;5;241m.\u001B[39mupdate_tracks([box[:\u001B[38;5;241m4\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m box \u001B[38;5;129;01min\u001B[39;00m face_boxes], frame\u001B[38;5;241m=\u001B[39mframe)\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m track \u001B[38;5;129;01min\u001B[39;00m tracks:\n\u001B[1;32m     65\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m track\u001B[38;5;241m.\u001B[39mis_confirmed():\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/deep_sort_realtime/deepsort_tracker.py:195\u001B[0m, in \u001B[0;36mDeepSort.update_tracks\u001B[0;34m(self, raw_detections, embeds, frame, today, others, instance_masks)\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(raw_detections) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m: \n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolygon:\n\u001B[0;32m--> 195\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(raw_detections[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m4\u001B[39m\n\u001B[1;32m    196\u001B[0m         raw_detections \u001B[38;5;241m=\u001B[39m [d \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m raw_detections \u001B[38;5;28;01mif\u001B[39;00m d[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m d[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tracking only for one specific class -  humans (index 0)\n",
    "# just detection for other objects \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sort import Sort\n",
    "\n",
    "\n",
    "class Detector:\n",
    "    def __init__(self, capture_index):\n",
    "        self.capture_index = capture_index\n",
    "        self.model = self.load_model()\n",
    "        self.names = list(self.model.model.names.values())\n",
    "        self.device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "    def load_model(self):\n",
    "        model = YOLO(\"yolov8m.pt\")\n",
    "        model.fuse()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict(self, frame):\n",
    "        results = self.model(frame, verbose=True)\n",
    "        return results\n",
    "\n",
    "    def get_results(self, results):\n",
    "        detections_list = []\n",
    "        for result in results[0]:\n",
    "\n",
    "            class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            # results, which will be fit into \"sort\" must be only of index 0 - so we choose only humans for tracking\n",
    "            if class_id == 0:\n",
    "                bboxes = result.boxes.xyxy.cpu().numpy()\n",
    "                score = result.boxes.conf.cpu().numpy()\n",
    "                class_id = result.boxes.cls.cpu().numpy()\n",
    "\n",
    "                detection = [bboxes[0][0], bboxes[0][1], bboxes[0][2], bboxes[0][3], score[0], class_id[0]]\n",
    "                detections_list.append(detection)\n",
    "\n",
    "        return np.array(detections_list)\n",
    "\n",
    "    def draw(self, frame, bboxes, score, class_id):\n",
    "        \n",
    "        \n",
    "        for bbox, conf, obj_id in zip(bboxes, score, class_id):\n",
    "            \n",
    "            label = f\"{conf}:  {self.names[int(obj_id)]}\"\n",
    "            \n",
    "            cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, label, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        return frame\n",
    "    \n",
    "    \n",
    "    def draw_without_id(self, frame, results): \n",
    "        \n",
    "        bboxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\n",
    "        class_id = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "        \n",
    "        for bbox, obj_id in zip(bboxes, class_id):\n",
    "            name = self.names[int(obj_id)]\n",
    "            \n",
    "            if obj_id != 0:\n",
    "                random.seed(int(obj_id))\n",
    "                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "                cv2.putText(frame, name, (int(bbox[0]), int(bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        return frame\n",
    "                \n",
    "\n",
    "    def __call__(self):\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        ret, frame = cap.read()\n",
    "        assert ret\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "        sort = Sort(max_age=100, min_hits=8, iou_threshold=0.50)\n",
    "\n",
    "        while True:\n",
    "            start_time = time()\n",
    "            ret, frame = cap.read()\n",
    "            assert ret\n",
    "\n",
    "            results = self.predict(frame)\n",
    "            detections_list = self.get_results(results)\n",
    "            \n",
    "            if len(detections_list) == 0:\n",
    "                detections_list = np.empty((0, 5))\n",
    "\n",
    "            res = sort.update(detections_list)\n",
    "            boxes_track = res[:, :-1]\n",
    "            boxes_ids = res[:, -1].astype(int)\n",
    "            class_id = detections_list[:, -1]\n",
    "\n",
    "            frame = self.draw(frame, boxes_track, boxes_ids, class_id)\n",
    "            frame = self.draw_without_id(frame, results)\n",
    "\n",
    "            end_time = time()\n",
    "            fps = 1 / np.round(end_time - start_time, 2)\n",
    "            cv2.putText(frame, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow('YOLOv8 Detection', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "detector = Detector(capture_index=1)\n",
    "detector()\n",
    "\n",
    "\n"
   ],
   "id": "5295d80226174ad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Deep sort",
   "id": "c2e2f47867fbb432"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from time import time  \n",
    "from ultralytics import YOLO\n",
    "from deepsort_tracker import Tracker\n",
    "\n",
    "model = YOLO(\"yolov8m.pt\")\n",
    "model.fuse()\n",
    "# import tracker object from deepsort_tracker.py file \n",
    "tracker = Tracker()\n",
    "\n",
    "# set random colors to bounding boxers\n",
    "colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for _ in range(10)]\n",
    "\n",
    "detection_threshold = 0.3\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "assert cap.isOpened()\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "while True:\n",
    "    start_time = time()\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Exit the loop if there's no frame\n",
    "\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    \n",
    "    detections = []\n",
    "    for result in results.boxes.data.tolist():\n",
    "        \n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "        if score > detection_threshold:\n",
    "            detections.append([int(x1), int(y1), int(x2), int(y2), int(score)])\n",
    "        \n",
    "        # update the tracker\n",
    "        tracker.update(frame, detections)\n",
    "        # get coordinates from tracked objects \n",
    "        for track in tracker.tracks:\n",
    "            bbox = track.bbox\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            track_id = track.track_id\n",
    "\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), colors[track_id % len(colors)], 3)\n",
    "            cv2.putText(frame, \"ID: \" + str(track_id), (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    end_time = time()\n",
    "    fps = 1/np.round(end_time - start_time, 2)\n",
    "        \n",
    "    cv2.putText(frame, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "277f8dd4d301a9bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
